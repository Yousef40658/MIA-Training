# Task 5_2: Sensors and Autonomous Navigation 🤖

In this task, I explored different **sensors** used in robotics and autonomous navigation. I not only studied them theoretically but also **tested several in real life** 🛠️, which gave me a deeper understanding of how they work, their trade-offs, and when each type is preferred (especially since some are expensive 💰).  

### Key Learnings 📌
- **Sensor Types** 🔧  
  - **Lidar**: precise distance measurement and mapping 🗺️.  
  - **Ultrasonic sensors**: cost-effective distance measurement 📏.  
  - **BNO/IMU**: orientation, velocity, and rotation around three axes 🎛️.  
  - **Temperature, humidity, radiation, and gas sensors**: help determine whether the robot can safely operate in an environment 🌡️💨☢️ and for how long ⏳.  

- **Cameras** 📷  
  - Gained introductory knowledge of camera systems 🎥.  
  - Later used with **OpenCV** to detect objects 🟦, signs 🚦, and paths ➡️.  

### Autonomous Navigation Insights 🧭
- Robots must navigate **safe paths**, where environmental data (e.g., heat 🔥, humidity 💧, gas 💨, radiation ☢️) can **enable or disable specific functions**.  
- This ensures the robot can complete its mission 🎯 while still being able to **return safely** 🔄.  
- **IMU units** provide motion and rotation data depending on the robot’s degrees of freedom 🔄.  

### Advanced Applications 🚀
- **Camera-based detection**: useful for object retrieval 🎒 or following sign-marked paths 🛣️ (applied in competition 🏆).  
- **SLAM (Simultaneous Localization and Mapping)** 🗺️: achieved using **Lidar + wheel encoders + camera** to map the environment and avoid obstacles 🚧.  

### Outcome 🏁
- Tested and studied **multiple sensor types** in real life 🛠️.  
- Built a strong foundation in **robotics sensing technologies** ⚡.  
- Learned how combining multiple sensors improves **autonomous navigation** 🤝.  
- Gained insights into **SLAM systems** and real-world robotic competition applications 🏆.  
