# AI Research & Reflection ğŸ¤–ğŸ“š

This work focused on researching the mathematical foundations required for solid ML modeling rather than rushing to write lots of code documentation.  
I implemented models correctly and understood the code, but decided the time was better spent deepening math knowledge first â€” then Iâ€™ll come back and write detailed, accurate markdowns for each model. ğŸ”

---

## Highlights â€” topics studied ğŸ“Œ
- **Math foundations**: linear algebra, probability, and the math behind training choices. ğŸ“  
- **Data prep & analysis**: cleaning, handling NaNs, detecting skewness, bias, outliers, and feature correlations. ğŸ§¹ğŸ”  
- **Feature engineering**: when to drop low-relevance features vs. when they provide useful noise; encoding categorical values (e.g., male/female â†’ numbers); clipping extreme values; creating new features to improve models. ğŸ› ï¸  
- **Exploratory plotting**: using Matplotlib to visualize distributions, correlations, and model behavior. ğŸ“Š  
- **Data structures**: differences between plain Python lists-of-lists and NumPy arrays (performance & semantics). ğŸ§®  
- **Embeddings**: intuition for embeddings vs raw numbers (recommended explainer: 3Blue1Brown on transformers). ğŸŒ  
- **Algorithms practiced**:  
  - Built-from-scratch **Logistic Regression** (training, parameter tuning). ğŸ§   
  - Tried **KNN**, **LightGBM (LGB)**, and pre-built logistic regression. âš™ï¸  
  - Hyperparameter search methods: **grid search**, **random search**, and **Bayesian search**. ğŸ”

---

## Practical workflow notes ğŸ”§
- Always inspect data first: check missingness, skew, correlations, and suspicious IDs. âœ…  
- Replace vs remove NaNs: decide per-feature based on distribution and business logic. âš–ï¸  
- Negative sampling / adding varied backgrounds (for vision tasks) reduces overfitting. ğŸš«ğŸ¯  
- Small *k* in KNN â†’ higher overfitting risk; logistic regression can be more stable on smaller datasets. ğŸ“‰  
- Use visualizations to guide feature decisions before modeling. ğŸ–¼ï¸

---

## Outcome & plan ğŸ¯
- I wrote correct model code and understood its operation, but I rerouted time to **study math deeply** (see `research/` folder). ğŸ“‚  
- Next steps: finish math study â†’ revisit each model and write a specific, high-quality markdown per model with full understanding. ğŸ”âœ…  
---

## Reference ğŸ”—
- For embeddings intuition: 3Blue1Brown â€” *highly recommended*.  
- See the `research/` folder for detailed notes and readings.

