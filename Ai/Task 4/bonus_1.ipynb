{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd27fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b3e971",
   "metadata": {},
   "source": [
    "# Plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e9e3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot\n",
    "\n",
    "\n",
    "def print_weights(model):\n",
    "    for li, layer in enumerate(model.layers):\n",
    "        print(f\"Layer {li+1}:\")\n",
    "        for ni, neuron in enumerate(layer.neurons):\n",
    "            w_vals = [round(w.data, 4) for w in neuron.w]   # round for readability\n",
    "            b_val = round(neuron.b.data, 4)\n",
    "            print(f\"  Neuron {ni+1}: w={w_vals}, b={b_val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda73721",
   "metadata": {},
   "source": [
    "## VALUE CLASS\n",
    "1. Added log\n",
    "2. Added Relu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02f0cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Complete Value Class (Provided) ---\n",
    "class Value:\n",
    "  \n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._backward = lambda: None\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += 1.0 * out.grad\n",
    "      other.grad += 1.0 * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "      \n",
    "    return out\n",
    "  \n",
    "  def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    def _backward():\n",
    "        self.grad += other * (self.data ** (other - 1)) * out.grad\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __neg__(self): # -self\n",
    "        return self * -1\n",
    "\n",
    "  def __radd__(self, other): # other + self\n",
    "      return self + other\n",
    "\n",
    "  def __sub__(self, other): # self - other\n",
    "      return self + (-other)\n",
    "\n",
    "  def __rsub__(self, other): # other - self\n",
    "      return other + (-self)\n",
    "\n",
    "  def __rmul__(self, other): # other * self\n",
    "      return self * other\n",
    "\n",
    "  def __truediv__(self, other): # self / other\n",
    "      return self * other**-1\n",
    "\n",
    "  def __rtruediv__(self, other): # other / self\n",
    "      return other * self**-1\n",
    "\n",
    "  def tanh(self):\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "    out = Value(t, (self, ), 'tanh')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += (1 - t**2) * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def sigmoid(self):\n",
    "    x = self.data\n",
    "    t = 1.0 / (1.0 + math.exp(-x))\n",
    "    out = Value(t, (self, ), 'sigmoid')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += t * (1-t) * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x), (self, ), 'exp')\n",
    "    \n",
    "    def _backward():\n",
    "      self.grad += out.data * out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  ###################\n",
    "  def log(self) :\n",
    "     eps = 1e-8\n",
    "     x = self.data \n",
    "     x = max (x,eps)                        #to prevent log 0\n",
    "     out = Value(math.log(x),(self, ),'log')\n",
    "\n",
    "     def _backward():\n",
    "        self.grad += (1/x) * out.grad\n",
    "      \n",
    "     out._backward = _backward\n",
    "     return out\n",
    "  \n",
    "  def Relu(self) :\n",
    "     x = self.data \n",
    "     relu = max (0,x)                   #turns negatives to 0\n",
    "     out = Value(relu, (self,) , \"ReLU\")\n",
    "\n",
    "     def _backward() :\n",
    "        deriv = 0 if x <= 0 else 1\n",
    "        self.grad = deriv * out.grad\n",
    "      \n",
    "     out._backward = _backward\n",
    "     return out\n",
    "  \n",
    "  ####################\n",
    "  \n",
    "  def backward(self):\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(v)\n",
    "    build_topo(self)\n",
    "    \n",
    "    self.grad = 1.0\n",
    "    for node in reversed(topo):\n",
    "      node._backward()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eceb69e",
   "metadata": {},
   "source": [
    "## Building a Neural Network\n",
    "1. Added Kaiming (He)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c890748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Neuron(Module):\n",
    "  # TASK 3.1: Complete the Neuron class.\n",
    "  def __init__(self, nin):\n",
    "    self.fan_in = nin\n",
    "    #Best for relu\n",
    "    self.w = []\n",
    "    limit = math.sqrt(2.0 / self.fan_in)\n",
    "    self.w = [Value(random.uniform(-limit,limit)) for _ in range (self.fan_in)]\n",
    "    self.b = Value(0) #set bias to 0\n",
    "\n",
    "\n",
    "  def __call__(self, x):\n",
    "    act = sum((wi * xi for wi,xi in zip(self.w,x)), self.b)\n",
    "    return act.Relu()\n",
    "\n",
    "\n",
    "  \n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "class Layer(Module):\n",
    "  # TASK 3.2: Complete the Layer class.\n",
    "  def __init__(self, nin, nout , p = 0.2):\n",
    "    # A layer is a list of neurons.\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    self.probability = p  \n",
    "  \n",
    "  def __call__(self, x , training = True):\n",
    "    # When you call a layer, you call each neuron with the input x.\n",
    "    # outs = list of neuron outputs\n",
    "    # return outs[0] if len(outs) == 1 else outs\n",
    "    outs = []\n",
    "    for neuron in self.neurons :\n",
    "       outs.append(neuron(x))\n",
    "\n",
    "    if training :\n",
    "       dropped_n = []\n",
    "       for out in outs :\n",
    "          if random.random() < self.probability : #dropped neuron\n",
    "             dropped_n.append(Value(0))   \n",
    "\n",
    "          else :\n",
    "             dropped_n.append(out * (1 / (1 - self.probability)))\n",
    "        \n",
    "       outs = dropped_n   #scaled on neurons and 0 for off neurons\n",
    "             \n",
    "    return outs[0] if len(outs) == 1 else outs       \n",
    "       \n",
    "  \n",
    "  def parameters(self):\n",
    "    # A layer's parameters are all the parameters from all its neurons.\n",
    "    return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "  # TASK 3.3: Complete the MLP class.\n",
    "  def __init__(self, nin, nouts , p=0.2):\n",
    "    # 'nouts' is a list of layer sizes.\n",
    "    # e.g., MLP(3, [4, 4, 1]) is a network with 3 inputs, two hidden layers of 4 neurons, and one output neuron.\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "    pass\n",
    "  \n",
    "  def __call__(self, x , training = True):\n",
    "    # Pass the input 'x' through all the layers sequentially.\n",
    "    # TODO: Implement the forward pass through all layers.\n",
    "    for layer in self.layers :\n",
    "       x = layer(x , training=training)             #feeed the result of each layer to the next one\n",
    "       \n",
    "    return x\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6053f6e1",
   "metadata": {},
   "source": [
    "## Part 5: Capstone Project - Handwritten Digit Recognition\n",
    "\n",
    "\n",
    "### 5.1 Loading and Preparing the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51d44b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1437, 64)\n",
      "Test data shape: (360, 64)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1)) # Flatten images to 64-element vectors\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.2, shuffle=True, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "X_train /= 16.0\n",
    "X_test /= 16.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot = np.zeros((len(y), num_classes))\n",
    "    one_hot[np.arange(len(y)), y] = 1\n",
    "    return one_hot\n",
    "\n",
    "num_classes = 10\n",
    "y_train_one_hot = one_hot_encode(y_train, num_classes)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\") # 1437 samples, 64 features each\n",
    "print(f\"Test data shape: {X_test.shape}\")   # 360 samples, 64 features each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a928a",
   "metadata": {},
   "source": [
    "### 5.2 Building and Training the Digit Recognizer\n",
    "1. Added softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea579d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for batching (provided)\n",
    "def create_batches(X, y, batch_size):\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(X), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(X))\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        yield X[batch_indices], y[batch_indices]\n",
    "\n",
    "def softmax(values) :\n",
    "    exps = [v.exp() for v in values] \n",
    "    sum_exps = sum(exps,Value(0))\n",
    "\n",
    "    return [v/ sum_exps for v in exps] \n",
    "\n",
    "def cosine_lr(epoch, total_epochs, lr_max, lr_min=1e-4):\n",
    "    return lr_min + 0.5 * (lr_max - lr_min) * (1 + math.cos(math.pi * epoch / total_epochs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d95891d",
   "metadata": {},
   "source": [
    "# Training\n",
    "1. entropy loss\n",
    "2. Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81da2a",
   "metadata": {},
   "source": [
    "## Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10c4b9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.2, batch=64, beta=0.9, epochs=6 -> loss=0.7976, acc=94.44%\n",
      "lr=0.2, batch=64, beta=0.95, epochs=6 -> loss=0.9967, acc=87.78%\n",
      "lr=0.2, batch=64, beta=0.8, epochs=6 -> loss=0.8019, acc=96.11%\n",
      "lr=0.2, batch=64, beta=0.9, epochs=10 -> loss=0.6448, acc=95.56%\n",
      "lr=0.2, batch=64, beta=0.95, epochs=10 -> loss=1.5872, acc=53.89%\n",
      "lr=0.2, batch=64, beta=0.8, epochs=10 -> loss=0.7395, acc=96.39%\n",
      "lr=0.1, batch=64, beta=0.9, epochs=6 -> loss=0.8663, acc=95.28%\n",
      "lr=0.1, batch=64, beta=0.95, epochs=6 -> loss=0.8067, acc=95.28%\n",
      "lr=0.1, batch=64, beta=0.8, epochs=6 -> loss=0.8890, acc=95.28%\n",
      "lr=0.1, batch=64, beta=0.9, epochs=10 -> loss=0.7534, acc=97.50%\n",
      "lr=0.1, batch=64, beta=0.95, epochs=10 -> loss=0.9127, acc=95.28%\n",
      "lr=0.1, batch=64, beta=0.8, epochs=10 -> loss=0.7765, acc=95.56%\n",
      "lr=0.05, batch=64, beta=0.9, epochs=6 -> loss=1.3604, acc=67.78%\n",
      "lr=0.05, batch=64, beta=0.95, epochs=6 -> loss=1.1514, acc=81.39%\n",
      "lr=0.05, batch=64, beta=0.8, epochs=6 -> loss=1.6005, acc=59.44%\n",
      "lr=0.05, batch=64, beta=0.9, epochs=10 -> loss=1.0167, acc=84.72%\n",
      "lr=0.05, batch=64, beta=0.95, epochs=10 -> loss=1.1482, acc=75.83%\n",
      "lr=0.05, batch=64, beta=0.8, epochs=10 -> loss=1.2057, acc=77.22%\n",
      "lr=0.01, batch=64, beta=0.9, epochs=6 -> loss=1.8565, acc=65.00%\n",
      "lr=0.01, batch=64, beta=0.95, epochs=6 -> loss=1.7244, acc=62.50%\n",
      "lr=0.01, batch=64, beta=0.8, epochs=6 -> loss=2.0365, acc=56.94%\n",
      "lr=0.01, batch=64, beta=0.9, epochs=10 -> loss=1.7982, acc=52.78%\n",
      "lr=0.01, batch=64, beta=0.95, epochs=10 -> loss=1.7407, acc=48.89%\n",
      "lr=0.01, batch=64, beta=0.8, epochs=10 -> loss=2.0262, acc=52.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x70c5a730dfd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hodhod/miniconda3/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr=0.001, batch=64, beta=0.9, epochs=6 -> loss=2.2992, acc=25.56%\n",
      "lr=0.001, batch=64, beta=0.95, epochs=6 -> loss=2.2982, acc=32.78%\n",
      "lr=0.001, batch=64, beta=0.8, epochs=6 -> loss=2.3594, acc=7.78%\n",
      "lr=0.001, batch=64, beta=0.9, epochs=10 -> loss=2.2470, acc=38.33%\n",
      "lr=0.001, batch=64, beta=0.95, epochs=10 -> loss=2.2540, acc=33.61%\n",
      "lr=0.001, batch=64, beta=0.8, epochs=10 -> loss=2.3021, acc=32.22%\n",
      "\n",
      "Best loss config: (0.2, 64, 0.9, 10) with loss: 0.6447675924179266\n",
      "Best accuracy config: (0.1, 64, 0.9, 10) with accuracy: 97.50%\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.2,0.1,0.05,0.01,0.001]\n",
    "batch_sizes = [64]\n",
    "betas = [0.9,0.95,0.8]\n",
    "epochs = [6,10]\n",
    "\n",
    "best_config = None\n",
    "best_loss = float(\"inf\")\n",
    "\n",
    "best_acc_config = None\n",
    "best_acc = 0.0\n",
    "\n",
    "\n",
    "for lr_init in learning_rates:\n",
    "    for batch_size in batch_sizes:\n",
    "        for n_epochs in epochs :\n",
    "            for beta in betas :\n",
    "            \n",
    "\n",
    "                # fresh model each run\n",
    "                neural_network = MLP(64, [32, 10])\n",
    "                velocities = {p: 0.0 for p in neural_network.parameters()}\n",
    "\n",
    "                lr = lr_init\n",
    "                prev_epoch_loss = 0\n",
    "                minimum_learning = 0.1\n",
    "                total_loss = 0\n",
    "\n",
    "                for epoch in range(n_epochs):\n",
    "                    epoch_loss = 0\n",
    "\n",
    "                    for X_batch, y_batch in create_batches(X_train, y_train_one_hot, batch_size):\n",
    "                        inputs = [list(map(Value, xrow)) for xrow in X_batch]\n",
    "                        logits_batch = [neural_network(x, training=True) for x in inputs]\n",
    "                        ypred_batch = [softmax(logits) for logits in logits_batch]\n",
    "\n",
    "                        true_labels = [list(map(Value, yrow)) for yrow in y_batch]\n",
    "\n",
    "                        # Cross entropy loss\n",
    "                        loss = sum(\n",
    "                            -(yt * yp.log())\n",
    "                            for ytrue, ypred in zip(true_labels, ypred_batch)\n",
    "                            for yt, yp in zip(ytrue, ypred)\n",
    "                        ) / len(X_batch)\n",
    "\n",
    "                        # Zero gradients\n",
    "                        for parameter in neural_network.parameters():\n",
    "                            parameter.grad = 0\n",
    "                        loss.backward()\n",
    "\n",
    "                        # Momentum update\n",
    "                        for parameter in neural_network.parameters():\n",
    "                            velocity = velocities[parameter]\n",
    "                            velocity = (beta * velocity) + parameter.grad\n",
    "                            velocities[parameter] = velocity\n",
    "                            parameter.data -= lr * velocity\n",
    "\n",
    "                        epoch_loss += loss.data\n",
    "\n",
    "                    # Early stop if loss stagnates\n",
    "                    if abs(epoch_loss - prev_epoch_loss) <= 0.005:\n",
    "                        break\n",
    "                    prev_epoch_loss = epoch_loss\n",
    "\n",
    "                    lr *= 0.95\n",
    "                    total_loss = epoch_loss / (len(X_train) / batch_size)\n",
    "\n",
    "                # ---- Evaluate on test set ----\n",
    "                correct = 0\n",
    "                total = len(X_test)\n",
    "                inputs = [list(map(Value, xrow)) for xrow in X_test]\n",
    "\n",
    "                for i in range(total):\n",
    "                    outputs = neural_network(inputs[i], training=False)\n",
    "                    predicted_class = np.argmax([p.data for p in outputs])\n",
    "                    if predicted_class == y_test[i]:\n",
    "                        correct += 1\n",
    "                accuracy = correct / total\n",
    "\n",
    "                print(f\"lr={lr_init}, batch={batch_size}, beta={beta}, epochs={n_epochs} -> loss={total_loss:.4f}, acc={accuracy*100:.2f}%\")\n",
    "\n",
    "                # Track best loss\n",
    "                if total_loss < best_loss:\n",
    "                    best_loss = total_loss\n",
    "                    best_loss_config = (lr_init, batch_size, beta, n_epochs)\n",
    "\n",
    "                # Track best accuracy\n",
    "                if accuracy > best_acc:\n",
    "                    best_acc = accuracy\n",
    "                    best_acc_config = (lr_init, batch_size, beta, n_epochs)\n",
    "\n",
    "print(\"\\nBest loss config:\", best_loss_config, \"with loss:\", best_loss)\n",
    "print(\"Best accuracy config:\", best_acc_config, f\"with accuracy: {best_acc*100:.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2029685a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 2410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x70c5a730dfd0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hodhod/miniconda3/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 781, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 2.2477\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I transfered this here so i can play this block individually, keeping the weights that the tunning operated on\n",
    "\"\"\"\n",
    "# Network architecture: 64 inputs -> 32 neuron hidden layer -> 10 output neurons\n",
    "# You can try changing the architecture, but this is a good starting point.\n",
    "neural_network = MLP(64, [32, 10])\n",
    "print(f\"Number of parameters: {len(neural_network.parameters())}\")\n",
    "\n",
    "\n",
    "\n",
    "# TASK 5.1: Complete the training loop for the digit recognizer.\n",
    "#decreased learning rate , increased batch size and n_epochs to experiment\n",
    "#losses kept same values due to big learning rate (1) , exploding\n",
    "lr = 0.2\n",
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "beta = 0.9\n",
    "velocities = {p : 0.0 for p in neural_network.parameters()} #initialzing to zero\n",
    "\n",
    "\n",
    "learning = True \n",
    "minimum_learning = 0.005\n",
    "\n",
    "\n",
    "prev_epoch_loss = 0 \n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in create_batches(X_train, y_train_one_hot, batch_size):\n",
    "        \n",
    "        # 1. Forward Pass\n",
    "        # TODO: Get predictions for each sample in the batch\n",
    "        # Note: The network expects inputs as lists of Value objects\n",
    "        inputs = [list(map(Value, xrow)) for xrow in X_batch]\n",
    "        logits_batch = [neural_network(x, training= True) for x in inputs]\n",
    "        ypred_batch = [softmax(logits) for logits in logits_batch ]\n",
    "\n",
    "        # 2. Compute Loss for the batch\n",
    "        # TODO: Calculate the MSE loss for the batch\n",
    "        # The true labels `y_batch` also need to be converted to Value objects for the subtraction\n",
    "        true_lables = [list(map(Value,yrow)) for yrow in y_batch]\n",
    "        \n",
    "        loss = sum(\n",
    "        -(yt * yp.log())\n",
    "        for ytrue, ypred in zip(true_lables, ypred_batch)\n",
    "        for yt, yp in zip(ytrue, ypred)\n",
    "        ) / len(X_batch)                                #to compare between different batch sizes\n",
    "        \n",
    "        # 3. Backward Pass\n",
    "        # TODO: Zero gradients and backpropagate\n",
    "        for parameter in neural_network.parameters():\n",
    "             parameter.grad = 0 \n",
    "        loss.backward()\n",
    "\n",
    "        #momentum\n",
    "        for parameter in neural_network.parameters():\n",
    "             velocity = velocities[parameter]\n",
    "             velocity = (beta * velocity) + parameter.grad\n",
    "             velocities[parameter] = velocity\n",
    "             parameter.data -= lr * velocity\n",
    "\n",
    "        epoch_loss += loss.data\n",
    "    if abs(epoch_loss - prev_epoch_loss) < minimum_learning :\n",
    "        break \n",
    "    prev_epoch_loss = epoch_loss\n",
    "\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Average Loss: {epoch_loss / (len(X_train)/batch_size):.4f}\")\n",
    "    # Decrease learning rate over time\n",
    "    lr *= 0.95  #stablizes training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe4a0a",
   "metadata": {},
   "source": [
    "### 5.3 Evaluating the Network\n",
    "\n",
    "Let's see how well our trained model performs on the test set!\n",
    "\n",
    "*(Evaluation code is provided for you)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26c7d4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Accuracy: 88.06%\n",
      "\n",
      "Layer 1:\n",
      "  Neuron 1: w=[np.float64(0.0102), np.float64(0.0873), np.float64(-0.0117), np.float64(0.219), np.float64(-0.1304), np.float64(-0.0808), np.float64(-0.1117), np.float64(0.0739), np.float64(0.1428), np.float64(0.0256), np.float64(-0.2564), np.float64(-0.0878), np.float64(0.1067), np.float64(-0.1169), np.float64(0.007), np.float64(0.1225), np.float64(-0.0222), np.float64(-0.0357), np.float64(0.0385), np.float64(0.191), np.float64(0.2584), np.float64(-0.14), np.float64(-0.1203), np.float64(-0.0629), np.float64(0.0657), np.float64(0.0122), np.float64(-0.0177), np.float64(0.3144), np.float64(0.1728), np.float64(-0.1968), np.float64(0.1021), np.float64(0.0041), np.float64(-0.0053), np.float64(-0.0898), np.float64(0.0515), np.float64(0.1749), np.float64(0.0924), np.float64(-0.0774), np.float64(-0.1101), np.float64(-0.044), np.float64(-0.0644), np.float64(-0.0548), np.float64(-0.0279), np.float64(0.1101), np.float64(0.0542), np.float64(-0.2015), np.float64(-0.2297), np.float64(-0.1219), np.float64(-0.1404), np.float64(-0.0382), np.float64(-0.0351), np.float64(0.1306), np.float64(-0.0007), np.float64(-0.1713), np.float64(-0.0655), np.float64(0.2135), np.float64(0.1487), np.float64(0.0375), np.float64(-0.2348), np.float64(0.0279), np.float64(0.0035), np.float64(-0.2645), np.float64(-0.0327), np.float64(-0.0536)], b=-0.0285\n",
      "  Neuron 2: w=[np.float64(0.1363), np.float64(-0.0499), np.float64(-0.125), np.float64(0.1603), np.float64(-0.292), np.float64(-0.3308), np.float64(-0.1022), np.float64(0.0645), np.float64(0.1594), np.float64(-0.4046), np.float64(0.0072), np.float64(0.2195), np.float64(0.2306), np.float64(0.2592), np.float64(-0.0913), np.float64(0.0665), np.float64(-0.1303), np.float64(-0.1036), np.float64(0.5508), np.float64(0.0796), np.float64(-0.8928), np.float64(0.1351), np.float64(-0.1915), np.float64(-0.0768), np.float64(0.0881), np.float64(0.2885), np.float64(0.3376), np.float64(-0.3881), np.float64(-1.0121), np.float64(-0.1955), np.float64(-0.0953), np.float64(0.0017), np.float64(-0.1645), np.float64(0.38), np.float64(0.5526), np.float64(-0.236), np.float64(-0.8324), np.float64(-0.1975), np.float64(0.1316), np.float64(0.0496), np.float64(0.0972), np.float64(-0.221), np.float64(0.7429), np.float64(-0.6662), np.float64(-0.6354), np.float64(0.097), np.float64(0.1484), np.float64(-0.0321), np.float64(-0.129), np.float64(-0.3338), np.float64(0.529), np.float64(0.0963), np.float64(-0.0136), np.float64(0.2237), np.float64(0.0553), np.float64(-0.0929), np.float64(-0.1045), np.float64(0.0266), np.float64(-0.488), np.float64(0.1395), np.float64(-0.1735), np.float64(-0.0166), np.float64(-0.1257), np.float64(0.0418)], b=-0.2112\n",
      "  Neuron 3: w=[np.float64(-0.061), np.float64(-0.0507), np.float64(0.0815), np.float64(-0.154), np.float64(-0.1047), np.float64(-0.1179), np.float64(-0.1718), np.float64(0.0246), np.float64(-0.1091), np.float64(-0.0177), np.float64(-0.174), np.float64(0.0745), np.float64(-0.0015), np.float64(-0.0056), np.float64(-0.1178), np.float64(-0.0369), np.float64(0.0061), np.float64(-0.0519), np.float64(-0.0148), np.float64(-0.1365), np.float64(0.0036), np.float64(0.0794), np.float64(0.0592), np.float64(0.1654), np.float64(-0.0409), np.float64(-0.0573), np.float64(-0.1389), np.float64(-0.064), np.float64(-0.0627), np.float64(-0.1844), np.float64(-0.1457), np.float64(-0.0501), np.float64(0.1435), np.float64(0.1333), np.float64(0.0823), np.float64(0.1037), np.float64(0.0458), np.float64(-0.1733), np.float64(0.0841), np.float64(-0.0968), np.float64(-0.0858), np.float64(0.1111), np.float64(-0.117), np.float64(-0.1062), np.float64(0.0324), np.float64(-0.1317), np.float64(-0.0837), np.float64(0.0276), np.float64(-0.0958), np.float64(0.0882), np.float64(0.1032), np.float64(-0.0066), np.float64(-0.1793), np.float64(0.0658), np.float64(0.0713), np.float64(0.1433), np.float64(-0.1399), np.float64(0.1641), np.float64(-0.0649), np.float64(-0.1525), np.float64(-0.2162), np.float64(-0.045), np.float64(-0.1937), np.float64(0.14)], b=-0.0855\n",
      "  Neuron 4: w=[np.float64(-0.1659), np.float64(-0.078), np.float64(-0.2425), np.float64(0.0187), np.float64(-0.5416), np.float64(-0.8945), np.float64(-0.0766), np.float64(0.0859), np.float64(-0.1482), np.float64(-0.0379), np.float64(-0.1204), np.float64(-0.2335), np.float64(-0.0569), np.float64(0.2938), np.float64(0.0311), np.float64(-0.1135), np.float64(-0.0938), np.float64(0.0671), np.float64(0.1303), np.float64(0.0457), np.float64(0.6211), np.float64(1.0093), np.float64(0.2467), np.float64(-0.1591), np.float64(-0.0949), np.float64(0.1903), np.float64(0.3624), np.float64(0.275), np.float64(0.448), np.float64(1.0136), np.float64(0.4115), np.float64(-0.1125), np.float64(-0.0647), np.float64(0.0657), np.float64(-0.4552), np.float64(-0.0083), np.float64(-0.3501), np.float64(0.3722), np.float64(0.3426), np.float64(-0.0422), np.float64(-0.1635), np.float64(0.1895), np.float64(-1.0031), np.float64(-0.6141), np.float64(-0.1146), np.float64(0.0415), np.float64(-0.0039), np.float64(-0.1611), np.float64(0.0043), np.float64(-0.088), np.float64(-0.5533), np.float64(-0.7424), np.float64(-0.1497), np.float64(-0.2074), np.float64(0.0149), np.float64(0.1084), np.float64(0.0732), np.float64(-0.039), np.float64(-0.3059), np.float64(-0.0775), np.float64(0.1151), np.float64(0.0534), np.float64(0.0454), np.float64(0.1656)], b=-0.0139\n",
      "  Neuron 5: w=[np.float64(0.0321), np.float64(0.1694), np.float64(0.4159), np.float64(0.1858), np.float64(-0.1477), np.float64(0.4011), np.float64(0.5177), np.float64(0.103), np.float64(0.0341), np.float64(-0.0349), np.float64(-0.2479), np.float64(0.3632), np.float64(0.1667), np.float64(-0.6466), np.float64(-0.1373), np.float64(0.0512), np.float64(-0.1312), np.float64(-0.2747), np.float64(0.3042), np.float64(0.142), np.float64(-0.9892), np.float64(-1.0991), np.float64(-0.5655), np.float64(0.0054), np.float64(0.1162), np.float64(0.1411), np.float64(0.2002), np.float64(-0.3825), np.float64(0.1707), np.float64(-0.2091), np.float64(-0.4902), np.float64(0.1576), np.float64(-0.1734), np.float64(0.0809), np.float64(0.0551), np.float64(-0.2307), np.float64(0.1797), np.float64(-0.3078), np.float64(-0.3562), np.float64(-0.1316), np.float64(-0.0896), np.float64(-0.3), np.float64(0.0463), np.float64(0.5722), np.float64(-0.1741), np.float64(-0.6996), np.float64(-0.1845), np.float64(-0.0621), np.float64(-0.0162), np.float64(0.0199), np.float64(0.0144), np.float64(0.8617), np.float64(0.5581), np.float64(0.2128), np.float64(0.044), np.float64(-0.0302), np.float64(0.0993), np.float64(-0.0785), np.float64(0.4634), np.float64(0.2756), np.float64(0.0724), np.float64(0.3928), np.float64(0.3957), np.float64(0.4528)], b=-0.0659\n",
      "  Neuron 6: w=[np.float64(0.1703), np.float64(0.1485), np.float64(0.0991), np.float64(0.1501), np.float64(0.0444), np.float64(-0.169), np.float64(-0.1164), np.float64(0.0555), np.float64(0.1287), np.float64(0.1164), np.float64(-0.1361), np.float64(0.1141), np.float64(-0.1698), np.float64(0.0362), np.float64(-0.0519), np.float64(0.1486), np.float64(0.0354), np.float64(0.1542), np.float64(-0.1668), np.float64(-0.0877), np.float64(-0.0347), np.float64(-0.0119), np.float64(-0.0703), np.float64(-0.047), np.float64(-0.102), np.float64(-0.1262), np.float64(-0.0953), np.float64(-0.0693), np.float64(-0.0536), np.float64(-0.1532), np.float64(0.1372), np.float64(0.0038), np.float64(-0.1182), np.float64(-0.1575), np.float64(0.0776), np.float64(-0.154), np.float64(-0.0271), np.float64(-0.0176), np.float64(-0.0116), np.float64(0.1595), np.float64(0.1424), np.float64(0.1587), np.float64(-0.1003), np.float64(-0.0823), np.float64(-0.064), np.float64(0.0128), np.float64(0.1386), np.float64(0.1684), np.float64(-0.0721), np.float64(0.0794), np.float64(0.1322), np.float64(-0.1495), np.float64(-0.0459), np.float64(-0.0003), np.float64(-0.1289), np.float64(0.0143), np.float64(0.1722), np.float64(-0.027), np.float64(-0.0583), np.float64(0.165), np.float64(-0.0977), np.float64(-0.1623), np.float64(0.0314), np.float64(0.162)], b=-0.0035\n",
      "  Neuron 7: w=[np.float64(0.0603), np.float64(0.0145), np.float64(0.6278), np.float64(0.0025), np.float64(-0.1518), np.float64(0.1095), np.float64(0.3734), np.float64(0.027), np.float64(0.1003), np.float64(0.0244), np.float64(0.2033), np.float64(0.0505), np.float64(-0.35), np.float64(-0.3286), np.float64(-0.4575), np.float64(-0.1866), np.float64(0.1272), np.float64(-0.077), np.float64(0.5996), np.float64(0.0081), np.float64(-0.8741), np.float64(-0.2678), np.float64(-0.4842), np.float64(0.0917), np.float64(-0.0572), np.float64(0.1714), np.float64(0.9351), np.float64(0.3301), np.float64(0.1324), np.float64(0.2795), np.float64(0.128), np.float64(0.0808), np.float64(0.0952), np.float64(0.3853), np.float64(0.5777), np.float64(-0.229), np.float64(-0.2284), np.float64(0.386), np.float64(0.7055), np.float64(0.1636), np.float64(-0.0125), np.float64(-0.2211), np.float64(-0.6344), np.float64(-0.3751), np.float64(-0.2051), np.float64(-0.1444), np.float64(0.3535), np.float64(0.1591), np.float64(-0.12), np.float64(-0.1559), np.float64(-0.32), np.float64(0.0306), np.float64(-0.3166), np.float64(-0.4404), np.float64(0.0159), np.float64(0.0833), np.float64(-0.1206), np.float64(0.0866), np.float64(0.6299), np.float64(-0.0107), np.float64(-0.4973), np.float64(-0.2731), np.float64(-0.1472), np.float64(0.0922)], b=0.0624\n",
      "  Neuron 8: w=[np.float64(-0.0239), np.float64(0.1435), np.float64(0.2202), np.float64(0.1613), np.float64(-0.1085), np.float64(-0.479), np.float64(-0.146), np.float64(-0.0127), np.float64(-0.1128), np.float64(0.2771), np.float64(0.3231), np.float64(0.0491), np.float64(0.4089), np.float64(-0.1324), np.float64(-0.0966), np.float64(-0.1281), np.float64(-0.0687), np.float64(0.004), np.float64(0.1982), np.float64(-0.3133), np.float64(-0.3378), np.float64(0.2837), np.float64(0.1316), np.float64(0.0482), np.float64(-0.0915), np.float64(0.1397), np.float64(-0.6344), np.float64(-1.1284), np.float64(-0.6281), np.float64(0.1536), np.float64(0.1984), np.float64(0.136), np.float64(0.0873), np.float64(0.1524), np.float64(-0.3741), np.float64(-0.7009), np.float64(-0.4462), np.float64(-0.5967), np.float64(-0.0804), np.float64(0.1626), np.float64(-0.0209), np.float64(0.2759), np.float64(0.635), np.float64(0.8725), np.float64(-0.6128), np.float64(-0.525), np.float64(-0.5743), np.float64(-0.0498), np.float64(-0.1457), np.float64(0.1054), np.float64(0.329), np.float64(0.7658), np.float64(0.5177), np.float64(0.437), np.float64(-0.0165), np.float64(-0.1135), np.float64(-0.1634), np.float64(0.0896), np.float64(0.533), np.float64(-0.0624), np.float64(-0.1574), np.float64(0.4989), np.float64(0.674), np.float64(0.0443)], b=0.1366\n",
      "  Neuron 9: w=[np.float64(0.0988), np.float64(0.1202), np.float64(-0.3328), np.float64(-0.2008), np.float64(-0.3563), np.float64(-0.4234), np.float64(-0.2192), np.float64(-0.0814), np.float64(0.1096), np.float64(-0.2484), np.float64(-0.5506), np.float64(0.0035), np.float64(-0.0662), np.float64(-0.2845), np.float64(-0.2176), np.float64(-0.0133), np.float64(0.1278), np.float64(0.2038), np.float64(0.1568), np.float64(0.8056), np.float64(0.7336), np.float64(0.0411), np.float64(0.1505), np.float64(0.1917), np.float64(0.0336), np.float64(0.3939), np.float64(0.5136), np.float64(0.0625), np.float64(0.182), np.float64(0.285), np.float64(0.3291), np.float64(-0.0826), np.float64(-0.001), np.float64(0.5071), np.float64(0.1801), np.float64(-0.3647), np.float64(0.1029), np.float64(0.1204), np.float64(-0.0234), np.float64(-0.119), np.float64(0.1144), np.float64(0.2025), np.float64(-0.2562), np.float64(0.0912), np.float64(0.8434), np.float64(-0.2059), np.float64(-0.2253), np.float64(0.1145), np.float64(0.0694), np.float64(0.132), np.float64(-0.4661), np.float64(-0.295), np.float64(0.5545), np.float64(-0.5696), np.float64(-0.3765), np.float64(0.1944), np.float64(-0.0395), np.float64(0.0173), np.float64(-0.3814), np.float64(-0.0332), np.float64(0.2731), np.float64(-0.2783), np.float64(-0.2061), np.float64(0.0824)], b=-0.0852\n",
      "  Neuron 10: w=[np.float64(0.0944), np.float64(-0.2022), np.float64(-0.2155), np.float64(0.0562), np.float64(-0.6897), np.float64(0.369), np.float64(0.1236), np.float64(0.0382), np.float64(0.0077), np.float64(-0.2759), np.float64(-0.5736), np.float64(-0.3966), np.float64(0.3145), np.float64(0.2108), np.float64(-0.2272), np.float64(-0.0503), np.float64(0.0628), np.float64(-0.0192), np.float64(0.0351), np.float64(0.7401), np.float64(0.979), np.float64(-0.3534), np.float64(-0.343), np.float64(-0.0408), np.float64(0.0149), np.float64(0.0871), np.float64(0.3702), np.float64(0.5822), np.float64(0.4133), np.float64(0.1626), np.float64(-0.4273), np.float64(0.082), np.float64(0.1269), np.float64(-0.2752), np.float64(-0.1129), np.float64(-0.4709), np.float64(-0.0628), np.float64(0.1716), np.float64(-0.3186), np.float64(0.1494), np.float64(0.1654), np.float64(-0.3404), np.float64(-0.194), np.float64(0.2812), np.float64(-0.0066), np.float64(-0.4126), np.float64(-0.5397), np.float64(-0.1234), np.float64(-0.1563), np.float64(-0.3108), np.float64(-0.1905), np.float64(0.2576), np.float64(0.1084), np.float64(-0.3156), np.float64(-0.4134), np.float64(0.0117), np.float64(-0.1377), np.float64(-0.1569), np.float64(-0.3642), np.float64(-0.3225), np.float64(-0.161), np.float64(0.335), np.float64(-0.2278), np.float64(0.0061)], b=-0.5386\n",
      "  Neuron 11: w=[np.float64(0.0439), np.float64(-0.1069), np.float64(-0.3206), np.float64(-0.4789), np.float64(0.263), np.float64(-0.5013), np.float64(-0.3118), np.float64(-0.1419), np.float64(0.1232), np.float64(-0.036), np.float64(-0.4076), np.float64(-0.0183), np.float64(-0.3491), np.float64(-0.4436), np.float64(-0.1779), np.float64(-0.0534), np.float64(-0.0309), np.float64(-0.0079), np.float64(0.158), np.float64(0.2434), np.float64(-0.217), np.float64(-0.2297), np.float64(0.2002), np.float64(0.1173), np.float64(0.0528), np.float64(0.1049), np.float64(0.2715), np.float64(-0.103), np.float64(-0.2575), np.float64(-0.134), np.float64(0.2405), np.float64(0.0437), np.float64(-0.1417), np.float64(0.424), np.float64(0.4383), np.float64(-0.0273), np.float64(0.1432), np.float64(0.2199), np.float64(0.1953), np.float64(-0.01), np.float64(-0.0469), np.float64(0.369), np.float64(0.3985), np.float64(0.4181), np.float64(0.3639), np.float64(0.1356), np.float64(0.3054), np.float64(0.1567), np.float64(0.0492), np.float64(0.2286), np.float64(-0.1334), np.float64(-0.0086), np.float64(0.2274), np.float64(-0.0759), np.float64(0.0299), np.float64(0.0817), np.float64(0.1001), np.float64(0.0285), np.float64(-0.3624), np.float64(-0.261), np.float64(0.1036), np.float64(-0.0097), np.float64(-0.2028), np.float64(0.0444)], b=0.036\n",
      "  Neuron 12: w=[np.float64(0.1423), np.float64(0.0735), np.float64(-0.1317), np.float64(-0.4577), np.float64(-0.2002), np.float64(-0.7724), np.float64(-0.3643), np.float64(0.0539), np.float64(-0.084), np.float64(-0.2947), np.float64(-0.6857), np.float64(-0.1122), np.float64(-0.5146), np.float64(-0.5712), np.float64(-0.2137), np.float64(-0.0346), np.float64(-0.1752), np.float64(-0.0865), np.float64(-0.2884), np.float64(0.6588), np.float64(0.2914), np.float64(-0.3625), np.float64(-0.0449), np.float64(0.0467), np.float64(0.0375), np.float64(0.2688), np.float64(0.3697), np.float64(0.1256), np.float64(0.0932), np.float64(-0.0137), np.float64(0.4636), np.float64(0.1597), np.float64(0.1119), np.float64(0.3955), np.float64(0.0066), np.float64(-0.4068), np.float64(0.4398), np.float64(0.0016), np.float64(0.1761), np.float64(-0.1204), np.float64(-0.0565), np.float64(0.5295), np.float64(-0.035), np.float64(0.5064), np.float64(1.069), np.float64(0.091), np.float64(-0.2818), np.float64(-0.0539), np.float64(-0.1164), np.float64(0.3868), np.float64(-0.3324), np.float64(-0.0302), np.float64(0.8556), np.float64(-0.3931), np.float64(-0.2841), np.float64(0.0989), np.float64(-0.074), np.float64(-0.1607), np.float64(-0.3868), np.float64(-0.1368), np.float64(0.1433), np.float64(-0.1134), np.float64(0.2536), np.float64(0.2773)], b=-0.1334\n",
      "  Neuron 13: w=[np.float64(0.1119), np.float64(0.0424), np.float64(0.4681), np.float64(-0.1148), np.float64(0.0676), np.float64(-0.0296), np.float64(0.1112), np.float64(-0.2083), np.float64(-0.0269), np.float64(-0.0728), np.float64(0.0702), np.float64(0.0654), np.float64(0.0808), np.float64(-0.1377), np.float64(-0.4547), np.float64(-0.0842), np.float64(0.0225), np.float64(0.1989), np.float64(0.4709), np.float64(0.2122), np.float64(-0.3221), np.float64(0.0072), np.float64(-0.1028), np.float64(-0.0291), np.float64(0.1609), np.float64(0.5328), np.float64(0.8261), np.float64(0.0137), np.float64(-0.4148), np.float64(-0.0677), np.float64(0.2278), np.float64(-0.0349), np.float64(-0.029), np.float64(0.4279), np.float64(0.4668), np.float64(-0.6558), np.float64(-0.5171), np.float64(0.2108), np.float64(0.4549), np.float64(0.087), np.float64(-0.0232), np.float64(0.3455), np.float64(-0.3288), np.float64(-0.5773), np.float64(-0.085), np.float64(0.157), np.float64(0.062), np.float64(-0.1504), np.float64(0.1659), np.float64(0.0351), np.float64(-0.0061), np.float64(-0.2852), np.float64(0.5223), np.float64(-0.061), np.float64(-0.6691), np.float64(-0.0969), np.float64(0.0253), np.float64(0.1929), np.float64(0.1664), np.float64(0.0489), np.float64(0.1165), np.float64(-0.7821), np.float64(-0.3491), np.float64(-0.072)], b=0.0487\n",
      "  Neuron 14: w=[np.float64(0.1181), np.float64(-0.06), np.float64(-0.1314), np.float64(0.2242), np.float64(0.1064), np.float64(-0.711), np.float64(-0.1991), np.float64(0.1328), np.float64(-0.1487), np.float64(0.2878), np.float64(0.762), np.float64(-0.0696), np.float64(0.1239), np.float64(0.3591), np.float64(0.2015), np.float64(0.1379), np.float64(0.156), np.float64(0.1917), np.float64(0.09), np.float64(-0.5575), np.float64(0.0806), np.float64(1.052), np.float64(0.2838), np.float64(-0.1164), np.float64(-0.0807), np.float64(-0.2759), np.float64(-0.4727), np.float64(-0.1568), np.float64(-0.0187), np.float64(0.3096), np.float64(0.2441), np.float64(-0.0153), np.float64(0.1338), np.float64(-0.3133), np.float64(-0.8099), np.float64(0.1003), np.float64(-0.7287), np.float64(-0.8192), np.float64(-0.2017), np.float64(0.155), np.float64(-0.146), np.float64(0.0821), np.float64(0.0426), np.float64(-0.1477), np.float64(-0.7912), np.float64(-0.3016), np.float64(-0.0206), np.float64(-0.0205), np.float64(-0.0494), np.float64(0.2737), np.float64(0.5251), np.float64(0.0933), np.float64(0.0383), np.float64(0.6243), np.float64(0.2314), np.float64(-0.007), np.float64(-0.0972), np.float64(-0.0367), np.float64(-0.1497), np.float64(0.2873), np.float64(0.481), np.float64(0.1096), np.float64(0.2854), np.float64(0.0336)], b=-0.0425\n",
      "  Neuron 15: w=[np.float64(-0.1636), np.float64(-0.1265), np.float64(0.0874), np.float64(-0.2188), np.float64(-0.17), np.float64(-0.0257), np.float64(-0.1161), np.float64(0.1555), np.float64(-0.1593), np.float64(-0.0151), np.float64(-0.1538), np.float64(-0.1254), np.float64(-0.0168), np.float64(-0.1438), np.float64(-0.1568), np.float64(-0.149), np.float64(-0.0586), np.float64(0.0811), np.float64(-0.033), np.float64(-0.0375), np.float64(-0.1457), np.float64(-0.1978), np.float64(-0.0576), np.float64(0.0919), np.float64(-0.1703), np.float64(-0.1172), np.float64(-0.3466), np.float64(0.0534), np.float64(-0.124), np.float64(0.0653), np.float64(-0.1445), np.float64(-0.0605), np.float64(0.1471), np.float64(-0.1402), np.float64(-0.1191), np.float64(-0.1206), np.float64(-0.1338), np.float64(-0.0506), np.float64(-0.063), np.float64(-0.107), np.float64(0.0102), np.float64(0.1624), np.float64(-0.1724), np.float64(-0.1451), np.float64(0.0112), np.float64(-0.1941), np.float64(-0.0784), np.float64(0.0394), np.float64(0.059), np.float64(-0.1408), np.float64(-0.1376), np.float64(-0.035), np.float64(-0.0455), np.float64(-0.1754), np.float64(-0.2266), np.float64(0.0557), np.float64(0.1207), np.float64(0.1358), np.float64(0.0271), np.float64(-0.0662), np.float64(-0.0798), np.float64(-0.3278), np.float64(-0.0184), np.float64(0.1286)], b=-0.2284\n",
      "  Neuron 16: w=[np.float64(-0.0004), np.float64(0.1555), np.float64(-0.3413), np.float64(-0.3466), np.float64(0.2738), np.float64(-0.5086), np.float64(-0.2593), np.float64(0.1624), np.float64(0.1098), np.float64(0.2304), np.float64(-0.075), np.float64(-0.0074), np.float64(-0.6451), np.float64(-0.1831), np.float64(-0.07), np.float64(-0.1757), np.float64(-0.0089), np.float64(0.1445), np.float64(0.4025), np.float64(-0.1608), np.float64(-0.7408), np.float64(-0.0178), np.float64(0.1153), np.float64(0.0825), np.float64(-0.0524), np.float64(0.3625), np.float64(0.1744), np.float64(-0.1146), np.float64(-0.2676), np.float64(-0.1516), np.float64(0.0204), np.float64(-0.045), np.float64(0.1555), np.float64(0.2787), np.float64(0.2339), np.float64(0.1273), np.float64(0.232), np.float64(-0.0143), np.float64(0.1924), np.float64(0.0014), np.float64(-0.0027), np.float64(0.3363), np.float64(0.9181), np.float64(0.4247), np.float64(0.2744), np.float64(0.7568), np.float64(0.3106), np.float64(-0.086), np.float64(0.1196), np.float64(-0.0147), np.float64(0.1572), np.float64(-0.2114), np.float64(-0.4684), np.float64(0.2896), np.float64(-0.0161), np.float64(-0.0971), np.float64(0.1443), np.float64(-0.1546), np.float64(-0.5345), np.float64(-0.0301), np.float64(0.2964), np.float64(-0.3431), np.float64(-0.4765), np.float64(-0.1256)], b=0.0932\n",
      "  Neuron 17: w=[np.float64(0.115), np.float64(-0.0204), np.float64(-0.614), np.float64(-0.3284), np.float64(-0.0698), np.float64(-0.2187), np.float64(-0.0134), np.float64(-0.0465), np.float64(-0.12), np.float64(-0.2812), np.float64(-0.1873), np.float64(-0.075), np.float64(-0.4615), np.float64(-0.6928), np.float64(-0.1643), np.float64(0.1109), np.float64(0.14), np.float64(-0.2025), np.float64(0.2719), np.float64(0.0538), np.float64(-0.7822), np.float64(-0.7074), np.float64(-0.166), np.float64(0.1364), np.float64(-0.0456), np.float64(-0.157), np.float64(0.1499), np.float64(-0.0671), np.float64(-0.5651), np.float64(-0.5658), np.float64(-0.3955), np.float64(-0.033), np.float64(-0.1727), np.float64(0.0037), np.float64(0.5161), np.float64(0.199), np.float64(0.1555), np.float64(0.1216), np.float64(-0.1568), np.float64(-0.137), np.float64(0.0108), np.float64(-0.2875), np.float64(0.9954), np.float64(0.4483), np.float64(-0.4471), np.float64(0.1185), np.float64(0.6956), np.float64(0.0088), np.float64(0.0324), np.float64(-0.1281), np.float64(0.2464), np.float64(0.5718), np.float64(-0.6353), np.float64(0.467), np.float64(0.4356), np.float64(-0.2397), np.float64(0.0097), np.float64(-0.0854), np.float64(-0.3407), np.float64(-0.1408), np.float64(0.0335), np.float64(0.7099), np.float64(-0.2136), np.float64(-0.2892)], b=-0.2351\n",
      "  Neuron 18: w=[np.float64(0.1574), np.float64(-0.1321), np.float64(0.0493), np.float64(-0.0513), np.float64(-0.0303), np.float64(-0.1588), np.float64(0.1291), np.float64(-0.1772), np.float64(-0.1664), np.float64(0.0847), np.float64(-0.1554), np.float64(0.0211), np.float64(-0.0793), np.float64(0.0633), np.float64(-0.1792), np.float64(0.1296), np.float64(0.054), np.float64(-0.1336), np.float64(-0.0975), np.float64(0.0716), np.float64(-0.15), np.float64(0.0131), np.float64(-0.1623), np.float64(-0.0325), np.float64(-0.1416), np.float64(0.1642), np.float64(-0.1181), np.float64(-0.0004), np.float64(-0.0533), np.float64(-0.1628), np.float64(-0.1316), np.float64(0.1349), np.float64(-0.0489), np.float64(0.0303), np.float64(0.0993), np.float64(-0.1479), np.float64(0.0624), np.float64(0.0675), np.float64(0.0274), np.float64(0.0785), np.float64(0.097), np.float64(-0.1296), np.float64(0.0252), np.float64(-0.0157), np.float64(0.0058), np.float64(-0.1096), np.float64(-0.0047), np.float64(-0.144), np.float64(0.1086), np.float64(0.0964), np.float64(-0.1084), np.float64(-0.0664), np.float64(0.1006), np.float64(-0.1757), np.float64(-0.0999), np.float64(0.0859), np.float64(-0.1197), np.float64(-0.1396), np.float64(0.1281), np.float64(-0.0598), np.float64(0.0494), np.float64(-0.1506), np.float64(0.0129), np.float64(0.0836)], b=-0.0397\n",
      "  Neuron 19: w=[np.float64(0.0806), np.float64(0.1674), np.float64(0.1697), np.float64(-0.1613), np.float64(-0.1414), np.float64(0.091), np.float64(0.0369), np.float64(-0.0756), np.float64(0.1193), np.float64(-0.0998), np.float64(-0.169), np.float64(-0.0359), np.float64(0.0587), np.float64(-0.0054), np.float64(-0.1468), np.float64(0.1689), np.float64(-0.0441), np.float64(-0.0899), np.float64(-0.0427), np.float64(-0.0287), np.float64(-0.111), np.float64(0.0373), np.float64(-0.0026), np.float64(-0.0576), np.float64(0.031), np.float64(0.0367), np.float64(0.007), np.float64(0.0465), np.float64(0.0117), np.float64(-0.0984), np.float64(0.0064), np.float64(0.1608), np.float64(-0.0684), np.float64(-0.0734), np.float64(-0.0281), np.float64(0.0252), np.float64(0.0419), np.float64(-0.1648), np.float64(-0.1539), np.float64(-0.1264), np.float64(-0.1248), np.float64(-0.0146), np.float64(-0.1754), np.float64(-0.1426), np.float64(-0.1243), np.float64(-0.0625), np.float64(-0.0026), np.float64(-0.1605), np.float64(-0.0688), np.float64(0.0787), np.float64(0.0963), np.float64(-0.1324), np.float64(-0.0424), np.float64(-0.113), np.float64(0.1045), np.float64(-0.0247), np.float64(0.0047), np.float64(-0.0467), np.float64(-0.1215), np.float64(-0.0612), np.float64(0.1679), np.float64(0.1303), np.float64(-0.0373), np.float64(0.1346)], b=0.0\n",
      "  Neuron 20: w=[np.float64(0.0592), np.float64(0.135), np.float64(0.5476), np.float64(0.127), np.float64(0.4864), np.float64(0.5679), np.float64(0.4629), np.float64(-0.2479), np.float64(0.0351), np.float64(0.2137), np.float64(0.4127), np.float64(0.016), np.float64(-0.0743), np.float64(-0.0176), np.float64(0.1994), np.float64(-0.0646), np.float64(0.0974), np.float64(0.1744), np.float64(0.1413), np.float64(-0.3169), np.float64(-0.4359), np.float64(-0.6755), np.float64(-0.6024), np.float64(-0.0955), np.float64(-0.0137), np.float64(0.1543), np.float64(0.2068), np.float64(0.5439), np.float64(0.68), np.float64(-0.1836), np.float64(-0.4842), np.float64(-0.1654), np.float64(0.0265), np.float64(-0.3003), np.float64(-0.0222), np.float64(0.151), np.float64(-0.0334), np.float64(0.1041), np.float64(-0.235), np.float64(-0.0756), np.float64(-0.0491), np.float64(-0.1042), np.float64(-1.0143), np.float64(-1.0875), np.float64(-0.2529), np.float64(0.4637), np.float64(0.1651), np.float64(-0.1345), np.float64(0.0674), np.float64(0.0486), np.float64(-0.3173), np.float64(-0.6337), np.float64(-0.3386), np.float64(-0.2058), np.float64(0.0078), np.float64(0.0251), np.float64(0.0655), np.float64(0.103), np.float64(0.6051), np.float64(0.33), np.float64(0.0043), np.float64(-0.3952), np.float64(-0.3917), np.float64(-0.3766)], b=-0.0292\n",
      "  Neuron 21: w=[np.float64(0.0426), np.float64(-0.0782), np.float64(0.068), np.float64(-0.0025), np.float64(-0.1148), np.float64(-0.1409), np.float64(-0.0961), np.float64(-0.0053), np.float64(-0.0332), np.float64(-0.0523), np.float64(0.2166), np.float64(0.1856), np.float64(-0.0807), np.float64(-0.0793), np.float64(-0.0398), np.float64(0.1301), np.float64(-0.0856), np.float64(-0.0777), np.float64(-0.0554), np.float64(-0.183), np.float64(0.1938), np.float64(-0.0337), np.float64(0.0628), np.float64(-0.0869), np.float64(-0.0425), np.float64(-0.105), np.float64(-0.1683), np.float64(0.0053), np.float64(0.0487), np.float64(-0.0921), np.float64(-0.126), np.float64(-0.1313), np.float64(-0.0013), np.float64(-0.1695), np.float64(-0.1849), np.float64(-0.0411), np.float64(-0.1497), np.float64(0.0281), np.float64(0.0749), np.float64(0.1522), np.float64(0.0761), np.float64(-0.1517), np.float64(0.1054), np.float64(-0.0781), np.float64(0.0759), np.float64(0.1522), np.float64(0.109), np.float64(-0.0492), np.float64(0.1645), np.float64(0.1803), np.float64(0.1024), np.float64(0.153), np.float64(0.1213), np.float64(0.1144), np.float64(0.2435), np.float64(-0.0643), np.float64(0.1348), np.float64(0.0506), np.float64(0.184), np.float64(-0.1272), np.float64(-0.0774), np.float64(-0.0671), np.float64(-0.129), np.float64(0.0736)], b=0.0647\n",
      "  Neuron 22: w=[np.float64(-0.1628), np.float64(-0.1045), np.float64(-0.1191), np.float64(0.0843), np.float64(-0.1076), np.float64(-0.4384), np.float64(0.0388), np.float64(-0.1454), np.float64(-0.0815), np.float64(-0.1153), np.float64(0.0845), np.float64(0.1626), np.float64(-0.2717), np.float64(-0.2878), np.float64(0.1696), np.float64(-0.0984), np.float64(-0.0915), np.float64(0.1352), np.float64(0.0899), np.float64(-0.172), np.float64(-0.1126), np.float64(0.3088), np.float64(0.2507), np.float64(-0.0835), np.float64(-0.1271), np.float64(0.1122), np.float64(0.3293), np.float64(0.1319), np.float64(0.2419), np.float64(0.4105), np.float64(0.3408), np.float64(0.0761), np.float64(-0.1531), np.float64(0.1967), np.float64(0.0636), np.float64(0.1857), np.float64(-0.0066), np.float64(0.1383), np.float64(-0.0633), np.float64(0.0579), np.float64(-0.1104), np.float64(0.0139), np.float64(-0.4503), np.float64(-0.1302), np.float64(-0.1862), np.float64(-0.0302), np.float64(0.0347), np.float64(-0.0223), np.float64(0.0189), np.float64(0.1494), np.float64(-0.2622), np.float64(-0.3796), np.float64(-0.0718), np.float64(-0.4752), np.float64(-0.0464), np.float64(-0.0959), np.float64(-0.1713), np.float64(0.1249), np.float64(-0.1321), np.float64(-0.1189), np.float64(0.1746), np.float64(0.0319), np.float64(-0.1143), np.float64(-0.1442)], b=0.0846\n",
      "  Neuron 23: w=[np.float64(0.0321), np.float64(-0.179), np.float64(-0.3618), np.float64(-0.0149), np.float64(-0.1467), np.float64(-0.381), np.float64(-0.2224), np.float64(-0.1244), np.float64(0.0324), np.float64(-0.1136), np.float64(0.1347), np.float64(-0.0748), np.float64(-0.7917), np.float64(0.1545), np.float64(0.1997), np.float64(0.1613), np.float64(0.1253), np.float64(-0.0583), np.float64(0.5793), np.float64(0.2188), np.float64(-0.1354), np.float64(0.9318), np.float64(0.3803), np.float64(-0.065), np.float64(0.0818), np.float64(-0.4561), np.float64(0.2581), np.float64(1.1363), np.float64(0.3029), np.float64(0.2488), np.float64(-0.2653), np.float64(0.0952), np.float64(-0.1221), np.float64(-0.5599), np.float64(-0.3753), np.float64(0.6452), np.float64(-0.0285), np.float64(-0.2803), np.float64(-0.2765), np.float64(0.0283), np.float64(0.1693), np.float64(-0.2823), np.float64(-0.0028), np.float64(-0.5875), np.float64(0.0427), np.float64(0.104), np.float64(0.2062), np.float64(-0.0508), np.float64(-0.0363), np.float64(-0.0074), np.float64(0.027), np.float64(-0.6305), np.float64(-0.8597), np.float64(-0.1189), np.float64(0.2485), np.float64(-0.0922), np.float64(0.0331), np.float64(-0.1998), np.float64(-0.234), np.float64(0.1646), np.float64(0.5659), np.float64(0.0451), np.float64(-0.0921), np.float64(-0.0014)], b=-0.2162\n",
      "  Neuron 24: w=[np.float64(0.1615), np.float64(0.1966), np.float64(0.2103), np.float64(0.3542), np.float64(-0.067), np.float64(0.1554), np.float64(0.1154), np.float64(0.0499), np.float64(0.0512), np.float64(0.2674), np.float64(-0.2828), np.float64(-0.1915), np.float64(0.5563), np.float64(0.0787), np.float64(-0.0956), np.float64(-0.1588), np.float64(-0.0382), np.float64(-0.1932), np.float64(-0.8028), np.float64(-0.4283), np.float64(0.931), np.float64(0.0107), np.float64(-0.0662), np.float64(0.1515), np.float64(0.1699), np.float64(-0.0764), np.float64(-0.6341), np.float64(-0.6797), np.float64(0.2174), np.float64(0.0216), np.float64(-0.292), np.float64(0.0563), np.float64(-0.0885), np.float64(0.0531), np.float64(-0.4279), np.float64(-0.4618), np.float64(0.0982), np.float64(0.1459), np.float64(0.0541), np.float64(0.0818), np.float64(-0.0691), np.float64(0.0741), np.float64(-0.886), np.float64(-0.1289), np.float64(-0.0751), np.float64(-0.515), np.float64(-0.2001), np.float64(-0.0634), np.float64(-0.1296), np.float64(0.1559), np.float64(-0.145), np.float64(0.2847), np.float64(0.7445), np.float64(0.2926), np.float64(0.3585), np.float64(0.1542), np.float64(-0.0208), np.float64(0.1043), np.float64(0.4013), np.float64(0.0142), np.float64(0.2134), np.float64(0.7599), np.float64(0.601), np.float64(0.2697)], b=0.1144\n",
      "  Neuron 25: w=[np.float64(0.0289), np.float64(0.0565), np.float64(-0.4842), np.float64(0.119), np.float64(0.3983), np.float64(0.2683), np.float64(0.1394), np.float64(-0.0536), np.float64(-0.0492), np.float64(-0.1631), np.float64(-0.4406), np.float64(-0.2174), np.float64(-0.1609), np.float64(0.1779), np.float64(0.1422), np.float64(-0.126), np.float64(-0.0928), np.float64(-0.4449), np.float64(-0.7744), np.float64(-0.1846), np.float64(0.1964), np.float64(-0.9858), np.float64(-0.365), np.float64(-0.1886), np.float64(-0.0874), np.float64(0.025), np.float64(-0.4362), np.float64(-0.2409), np.float64(-0.2096), np.float64(-1.162), np.float64(-0.7845), np.float64(0.015), np.float64(0.0077), np.float64(0.0146), np.float64(-0.0392), np.float64(-0.4462), np.float64(0.7773), np.float64(0.8173), np.float64(-0.0984), np.float64(0.0695), np.float64(-0.0418), np.float64(-0.4823), np.float64(-0.1989), np.float64(-0.7507), np.float64(0.2385), np.float64(0.7488), np.float64(0.637), np.float64(0.0989), np.float64(-0.119), np.float64(-0.0633), np.float64(0.0941), np.float64(-0.0404), np.float64(0.2322), np.float64(0.5214), np.float64(0.644), np.float64(0.0036), np.float64(-0.0899), np.float64(-0.1034), np.float64(0.0623), np.float64(0.0011), np.float64(0.2908), np.float64(0.774), np.float64(-0.4483), np.float64(-0.4189)], b=0.1545\n",
      "  Neuron 26: w=[np.float64(0.0467), np.float64(-0.1511), np.float64(-0.2567), np.float64(-0.0851), np.float64(-0.3706), np.float64(0.1941), np.float64(-0.5619), np.float64(-0.2231), np.float64(0.0898), np.float64(-0.2209), np.float64(-0.2733), np.float64(-0.1576), np.float64(-0.6049), np.float64(0.4046), np.float64(0.0028), np.float64(-0.069), np.float64(-0.0999), np.float64(0.0724), np.float64(0.3986), np.float64(0.3243), np.float64(-0.203), np.float64(-0.1645), np.float64(-0.1097), np.float64(-0.024), np.float64(0.1033), np.float64(-0.1799), np.float64(0.159), np.float64(0.8687), np.float64(-0.0288), np.float64(-0.1986), np.float64(-0.7696), np.float64(-0.0179), np.float64(0.1207), np.float64(-0.4146), np.float64(-0.2364), np.float64(0.8858), np.float64(0.0835), np.float64(-0.5635), np.float64(-0.8675), np.float64(-0.1322), np.float64(-0.0071), np.float64(-0.668), np.float64(0.6635), np.float64(0.0698), np.float64(0.3061), np.float64(0.0496), np.float64(-0.3638), np.float64(0.1664), np.float64(-0.1451), np.float64(-0.4222), np.float64(0.3256), np.float64(-0.3572), np.float64(-0.2658), np.float64(0.323), np.float64(-0.3602), np.float64(-0.0178), np.float64(0.1521), np.float64(-0.19), np.float64(-0.5462), np.float64(0.2001), np.float64(0.4041), np.float64(0.1754), np.float64(-0.0905), np.float64(0.2576)], b=-0.5842\n",
      "  Neuron 27: w=[np.float64(0.125), np.float64(-0.0565), np.float64(-0.3185), np.float64(0.614), np.float64(0.2457), np.float64(-0.2825), np.float64(0.0644), np.float64(0.1445), np.float64(0.0831), np.float64(-0.2622), np.float64(0.2718), np.float64(0.3026), np.float64(0.4657), np.float64(0.6281), np.float64(0.1999), np.float64(0.1215), np.float64(-0.1089), np.float64(-0.0206), np.float64(-0.4318), np.float64(-0.5111), np.float64(0.0102), np.float64(1.0718), np.float64(0.1782), np.float64(-0.131), np.float64(0.1609), np.float64(0.0097), np.float64(-0.3468), np.float64(-0.4037), np.float64(-0.3825), np.float64(0.2114), np.float64(0.3675), np.float64(0.0414), np.float64(-0.1106), np.float64(-0.0478), np.float64(-0.0155), np.float64(-0.428), np.float64(-0.6625), np.float64(0.2422), np.float64(0.3002), np.float64(0.0836), np.float64(-0.1316), np.float64(-0.005), np.float64(0.1051), np.float64(-0.7417), np.float64(-0.4269), np.float64(0.1304), np.float64(0.239), np.float64(0.1326), np.float64(-0.0651), np.float64(-0.2419), np.float64(0.0206), np.float64(0.0621), np.float64(0.0938), np.float64(-0.0842), np.float64(0.0304), np.float64(0.0365), np.float64(-0.0239), np.float64(0.0947), np.float64(0.0182), np.float64(0.2203), np.float64(0.1387), np.float64(-0.2506), np.float64(-0.4899), np.float64(-0.0208)], b=0.1681\n",
      "  Neuron 28: w=[np.float64(0.1527), np.float64(-0.1267), np.float64(0.021), np.float64(0.072), np.float64(-0.1675), np.float64(-0.031), np.float64(-0.0592), np.float64(-0.1032), np.float64(0.0632), np.float64(-0.0059), np.float64(-0.153), np.float64(-0.0339), np.float64(0.118), np.float64(0.0267), np.float64(0.132), np.float64(-0.0385), np.float64(-0.0457), np.float64(-0.1634), np.float64(-0.1702), np.float64(0.0874), np.float64(-0.078), np.float64(-0.2183), np.float64(-0.1058), np.float64(0.0304), np.float64(0.1495), np.float64(-0.0997), np.float64(-0.0784), np.float64(0.0777), np.float64(0.0167), np.float64(0.0629), np.float64(-0.0927), np.float64(0.0152), np.float64(0.1236), np.float64(-0.096), np.float64(-0.1308), np.float64(-0.1725), np.float64(-0.029), np.float64(-0.1895), np.float64(-0.0781), np.float64(0.1148), np.float64(-0.0552), np.float64(-0.1523), np.float64(-0.1527), np.float64(-0.0053), np.float64(0.0966), np.float64(-0.1094), np.float64(0.0528), np.float64(-0.1646), np.float64(-0.0135), np.float64(-0.025), np.float64(0.0424), np.float64(0.0284), np.float64(-0.0985), np.float64(-0.0255), np.float64(0.0389), np.float64(-0.1317), np.float64(0.144), np.float64(-0.163), np.float64(0.0509), np.float64(-0.1624), np.float64(-0.1884), np.float64(-0.0335), np.float64(0.1833), np.float64(0.0674)], b=-0.0688\n",
      "  Neuron 29: w=[np.float64(-0.1525), np.float64(-0.0868), np.float64(-0.3025), np.float64(-0.274), np.float64(-0.0229), np.float64(0.3279), np.float64(-0.3902), np.float64(-0.0667), np.float64(0.1784), np.float64(0.2876), np.float64(-0.1813), np.float64(-0.4178), np.float64(-0.0287), np.float64(0.3329), np.float64(0.0779), np.float64(0.074), np.float64(-0.0718), np.float64(0.0292), np.float64(-0.3424), np.float64(-0.1906), np.float64(0.5638), np.float64(0.2267), np.float64(0.3454), np.float64(-0.0462), np.float64(0.0714), np.float64(-0.1964), np.float64(-0.7275), np.float64(-0.1323), np.float64(0.0513), np.float64(-0.5027), np.float64(-0.7626), np.float64(-0.1753), np.float64(0.1443), np.float64(-0.2799), np.float64(-0.6311), np.float64(0.4827), np.float64(0.702), np.float64(-0.2016), np.float64(-0.8154), np.float64(0.0738), np.float64(-0.1541), np.float64(-0.2978), np.float64(0.5451), np.float64(0.3963), np.float64(0.0597), np.float64(-0.1886), np.float64(-0.1908), np.float64(-0.0682), np.float64(-0.1556), np.float64(0.0038), np.float64(0.0785), np.float64(-0.0796), np.float64(0.3455), np.float64(0.7271), np.float64(0.3655), np.float64(-0.0756), np.float64(-0.0847), np.float64(0.1288), np.float64(-0.2953), np.float64(-0.0206), np.float64(0.4628), np.float64(0.3652), np.float64(0.3625), np.float64(0.1701)], b=-0.1448\n",
      "  Neuron 30: w=[np.float64(-0.123), np.float64(-0.1023), np.float64(-0.1902), np.float64(-0.0752), np.float64(-0.0373), np.float64(0.0554), np.float64(0.1065), np.float64(0.0257), np.float64(-0.1273), np.float64(-0.2151), np.float64(-0.1384), np.float64(-0.212), np.float64(0.0468), np.float64(-0.159), np.float64(0.1333), np.float64(-0.0116), np.float64(0.0785), np.float64(-0.0937), np.float64(-0.4829), np.float64(0.0291), np.float64(0.5669), np.float64(0.1738), np.float64(0.0804), np.float64(-0.1443), np.float64(0.0708), np.float64(0.0083), np.float64(0.1187), np.float64(-0.3463), np.float64(-0.0761), np.float64(0.4033), np.float64(0.297), np.float64(-0.0341), np.float64(0.0514), np.float64(0.1704), np.float64(0.2031), np.float64(0.1086), np.float64(0.5054), np.float64(0.5676), np.float64(0.2414), np.float64(-0.1185), np.float64(0.1714), np.float64(0.1795), np.float64(0.1236), np.float64(0.6888), np.float64(0.443), np.float64(-0.1067), np.float64(-0.1089), np.float64(-0.05), np.float64(0.1508), np.float64(0.2029), np.float64(-0.1889), np.float64(0.4125), np.float64(-0.2954), np.float64(-0.3315), np.float64(-0.2449), np.float64(0.0155), np.float64(-0.1284), np.float64(-0.1605), np.float64(0.0367), np.float64(-0.036), np.float64(-0.4637), np.float64(-0.3541), np.float64(-0.1877), np.float64(-0.1805)], b=0.1536\n",
      "  Neuron 31: w=[np.float64(-0.0662), np.float64(0.0498), np.float64(-0.0592), np.float64(-0.1491), np.float64(0.0436), np.float64(-0.1504), np.float64(-0.0372), np.float64(0.0115), np.float64(-0.0515), np.float64(0.0778), np.float64(-0.0381), np.float64(-0.0844), np.float64(-0.0395), np.float64(-0.0258), np.float64(-0.1354), np.float64(0.101), np.float64(0.022), np.float64(-0.0577), np.float64(-0.0434), np.float64(-0.1837), np.float64(-0.0483), np.float64(0.0132), np.float64(0.1091), np.float64(-0.0532), np.float64(0.0515), np.float64(-0.135), np.float64(0.0546), np.float64(0.145), np.float64(0.1163), np.float64(0.1032), np.float64(-0.0427), np.float64(0.0394), np.float64(-0.0628), np.float64(0.0594), np.float64(-0.189), np.float64(0.1148), np.float64(-0.0081), np.float64(-0.0102), np.float64(-0.2053), np.float64(-0.0338), np.float64(-0.0512), np.float64(-0.1608), np.float64(-0.0031), np.float64(-0.1563), np.float64(-0.1343), np.float64(0.0682), np.float64(0.0249), np.float64(0.09), np.float64(-0.1179), np.float64(-0.0409), np.float64(-0.0184), np.float64(-0.0401), np.float64(-0.1566), np.float64(-0.026), np.float64(-0.0698), np.float64(0.1364), np.float64(-0.1148), np.float64(0.0547), np.float64(-0.1453), np.float64(-0.201), np.float64(-0.1526), np.float64(-0.1796), np.float64(-0.0591), np.float64(-0.0185)], b=-0.0925\n",
      "  Neuron 32: w=[np.float64(-0.0663), np.float64(-0.0142), np.float64(0.3048), np.float64(0.3219), np.float64(0.8987), np.float64(0.7981), np.float64(0.3935), np.float64(-0.2247), np.float64(-0.0639), np.float64(0.2236), np.float64(0.4281), np.float64(0.3408), np.float64(0.2564), np.float64(0.0732), np.float64(0.1536), np.float64(0.0288), np.float64(-0.0439), np.float64(0.1409), np.float64(-0.7108), np.float64(-0.742), np.float64(-0.1435), np.float64(-1.0771), np.float64(-0.466), np.float64(-0.1825), np.float64(-0.1417), np.float64(0.1695), np.float64(-0.4368), np.float64(-0.1974), np.float64(0.3167), np.float64(-1.0508), np.float64(-0.6659), np.float64(-0.1742), np.float64(0.0711), np.float64(0.0829), np.float64(-0.0787), np.float64(-0.278), np.float64(0.2464), np.float64(0.0655), np.float64(-0.0019), np.float64(0.0819), np.float64(-0.1064), np.float64(0.1296), np.float64(-0.7164), np.float64(-1.1579), np.float64(-0.0937), np.float64(0.5794), np.float64(0.6091), np.float64(-0.104), np.float64(-0.058), np.float64(-0.1438), np.float64(-0.0569), np.float64(-0.2637), np.float64(0.4436), np.float64(-0.0867), np.float64(-0.0161), np.float64(0.0082), np.float64(-0.0918), np.float64(0.1373), np.float64(0.5), np.float64(0.3994), np.float64(0.0557), np.float64(-0.3399), np.float64(-0.2163), np.float64(-0.4793)], b=0.0701\n",
      "Layer 2:\n",
      "  Neuron 1: w=[np.float64(0.1459), np.float64(1.6132), np.float64(-0.1596), np.float64(-0.4441), np.float64(-0.7415), np.float64(-0.1828), np.float64(-0.0421), np.float64(0.6914), np.float64(-0.27), np.float64(-0.455), np.float64(-0.1423), np.float64(-0.7007), np.float64(0.7956), np.float64(0.2299), np.float64(-0.2448), np.float64(0.3397), np.float64(-0.2939), np.float64(-0.1671), np.float64(-0.1064), np.float64(-0.9329), np.float64(0.0824), np.float64(-0.2129), np.float64(-0.5158), np.float64(-0.7448), np.float64(-1.111), np.float64(-0.7756), np.float64(0.4575), np.float64(-0.1044), np.float64(-0.9353), np.float64(-0.496), np.float64(-0.2461), np.float64(-0.1674)], b=-0.4396\n",
      "  Neuron 2: w=[np.float64(0.4076), np.float64(-0.3872), np.float64(-0.0985), np.float64(0.7679), np.float64(0.6346), np.float64(-0.1958), np.float64(-0.5095), np.float64(-0.8621), np.float64(0.5388), np.float64(1.149), np.float64(-0.6755), np.float64(0.4569), np.float64(-0.2781), np.float64(-1.3042), np.float64(-0.2062), np.float64(-1.0274), np.float64(-0.5803), np.float64(-0.1782), np.float64(0.1909), np.float64(-0.961), np.float64(-0.0675), np.float64(-0.4649), np.float64(-0.3569), np.float64(0.2267), np.float64(0.337), np.float64(0.6499), np.float64(-0.2471), np.float64(-0.1314), np.float64(0.4729), np.float64(-0.086), np.float64(0.0155), np.float64(-1.1963)], b=-0.2498\n",
      "  Neuron 3: w=[np.float64(-0.2631), np.float64(-0.7331), np.float64(-0.1609), np.float64(-0.5187), np.float64(0.5799), np.float64(-0.0162), np.float64(-1.1738), np.float64(1.2641), np.float64(-0.8362), np.float64(-0.3046), np.float64(0.0824), np.float64(-0.4932), np.float64(-0.7513), np.float64(0.4974), np.float64(-0.0903), np.float64(-0.9226), np.float64(-0.2425), np.float64(0.1744), np.float64(0.1328), np.float64(-0.9265), np.float64(0.276), np.float64(-0.2272), np.float64(-1.3233), np.float64(0.5772), np.float64(-0.6582), np.float64(-0.6197), np.float64(-0.2157), np.float64(0.2444), np.float64(0.6066), np.float64(-0.2538), np.float64(-0.0478), np.float64(-0.561)], b=-0.2644\n",
      "  Neuron 4: w=[np.float64(-0.1628), np.float64(-0.2646), np.float64(-0.0103), np.float64(-0.4873), np.float64(-1.1693), np.float64(-0.198), np.float64(-1.1078), np.float64(-1.3188), np.float64(-0.5775), np.float64(-0.44), np.float64(-0.0033), np.float64(-0.5829), np.float64(-0.8808), np.float64(0.3184), np.float64(-0.0112), np.float64(-0.6497), np.float64(-0.5419), np.float64(-0.0342), np.float64(-0.1467), np.float64(0.5554), np.float64(0.2484), np.float64(-0.0561), np.float64(0.0458), np.float64(0.4691), np.float64(0.9173), np.float64(-0.2276), np.float64(0.1604), np.float64(-0.2049), np.float64(0.3229), np.float64(-0.437), np.float64(0.004), np.float64(0.8314)], b=-0.4344\n",
      "  Neuron 5: w=[np.float64(-0.1515), np.float64(-0.3072), np.float64(0.141), np.float64(0.2384), np.float64(-0.606), np.float64(0.1412), np.float64(0.1481), np.float64(-0.0584), np.float64(0.6784), np.float64(-0.3254), np.float64(0.9612), np.float64(0.9058), np.float64(0.4133), np.float64(-0.8285), np.float64(0.2301), np.float64(0.419), np.float64(-0.4989), np.float64(0.0035), np.float64(-0.1978), np.float64(-0.6336), np.float64(0.0431), np.float64(0.3839), np.float64(-0.1333), np.float64(-0.9357), np.float64(-0.4489), np.float64(-0.7718), np.float64(-0.9645), np.float64(0.1272), np.float64(-0.5646), np.float64(0.1795), np.float64(-0.1009), np.float64(-0.816)], b=-0.4913\n",
      "  Neuron 6: w=[np.float64(-0.0524), np.float64(-0.4793), np.float64(0.1935), np.float64(-1.0231), np.float64(0.7528), np.float64(-0.1576), np.float64(0.772), np.float64(-0.1408), np.float64(-0.4705), np.float64(-0.2454), np.float64(-0.3222), np.float64(-0.3981), np.float64(0.8512), np.float64(-0.5805), np.float64(0.0659), np.float64(-0.1988), np.float64(-0.4638), np.float64(-0.1187), np.float64(0.2253), np.float64(0.6746), np.float64(-0.1692), np.float64(-0.1856), np.float64(-0.3682), np.float64(-0.7847), np.float64(-0.0651), np.float64(-0.147), np.float64(-1.1549), np.float64(-0.1474), np.float64(-1.3176), np.float64(-0.7731), np.float64(0.0973), np.float64(0.7842)], b=-0.6282\n",
      "  Neuron 7: w=[np.float64(-0.3187), np.float64(0.4498), np.float64(-0.1021), np.float64(-0.7668), np.float64(0.4549), np.float64(-0.1526), np.float64(0.1931), np.float64(-0.0535), np.float64(-0.8609), np.float64(-0.4257), np.float64(0.4958), np.float64(-0.4188), np.float64(-0.747), np.float64(-0.7115), np.float64(-0.175), np.float64(0.5656), np.float64(1.4602), np.float64(0.2314), np.float64(-0.0317), np.float64(-0.9558), np.float64(-0.1939), np.float64(-0.2751), np.float64(0.0415), np.float64(-0.9254), np.float64(0.7587), np.float64(0.3142), np.float64(-0.9719), np.float64(0.1926), np.float64(-0.0685), np.float64(-0.347), np.float64(-0.058), np.float64(-0.7552)], b=-0.1911\n",
      "  Neuron 8: w=[np.float64(0.0906), np.float64(0.065), np.float64(-0.2134), np.float64(-0.0336), np.float64(-0.2304), np.float64(0.1382), np.float64(-0.0567), np.float64(-0.0408), np.float64(-0.0443), np.float64(-0.0201), np.float64(-0.1885), np.float64(0.1082), np.float64(0.0332), np.float64(-0.0188), np.float64(-0.021), np.float64(-0.0323), np.float64(-0.0165), np.float64(-0.1932), np.float64(-0.2046), np.float64(-0.1124), np.float64(0.1497), np.float64(0.0049), np.float64(-0.1881), np.float64(-0.172), np.float64(-0.223), np.float64(-0.0206), np.float64(-0.1724), np.float64(0.1075), np.float64(-0.2587), np.float64(-0.0924), np.float64(-0.0931), np.float64(-0.043)], b=-0.2032\n",
      "  Neuron 9: w=[np.float64(-0.0036), np.float64(-0.4534), np.float64(-0.1883), np.float64(-0.674), np.float64(-0.6993), np.float64(0.0625), np.float64(-0.9187), np.float64(-0.6804), np.float64(-0.3263), np.float64(-0.8202), np.float64(-0.6104), np.float64(-0.6845), np.float64(-0.4368), np.float64(0.0901), np.float64(0.1564), np.float64(0.368), np.float64(-0.8275), np.float64(-0.1043), np.float64(-0.01), np.float64(-0.0257), np.float64(-0.0046), np.float64(-0.1619), np.float64(0.6944), np.float64(-0.9026), np.float64(-0.3596), np.float64(0.8334), np.float64(-0.7186), np.float64(-0.0843), np.float64(0.5447), np.float64(-0.7483), np.float64(0.1418), np.float64(-0.4625)], b=1.1214\n",
      "  Neuron 10: w=[np.float64(-0.195), np.float64(-0.4558), np.float64(0.0311), np.float64(0.9463), np.float64(-0.5162), np.float64(-0.1859), np.float64(0.0957), np.float64(-0.9473), np.float64(-0.0656), np.float64(-0.0553), np.float64(-0.2533), np.float64(-0.5874), np.float64(-0.0136), np.float64(0.4641), np.float64(0.0477), np.float64(-0.7939), np.float64(-0.1559), np.float64(-0.0248), np.float64(0.118), np.float64(0.2335), np.float64(0.1524), np.float64(0.6953), np.float64(0.9454), np.float64(0.0457), np.float64(-0.6995), np.float64(-0.4544), np.float64(0.336), np.float64(0.1872), np.float64(-1.0515), np.float64(-0.7797), np.float64(0.1189), np.float64(-0.6846)], b=-0.2465\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    correct = 0\n",
    "    total = len(X_test)\n",
    "    \n",
    "    inputs = [list(map(Value, xrow)) for xrow in X_test]\n",
    "    \n",
    "    for i in range(total):\n",
    "        # Get prediction from the network\n",
    "        outputs = model(inputs[i] , training = False)\n",
    "        predicted_class = np.argmax([p.data for p in outputs])\n",
    "        \n",
    "        if predicted_class == y_test[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\\n\")\n",
    "    print_weights(neural_network)\n",
    "\n",
    "\n",
    "evaluate_model(neural_network, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58b96bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_sample_with_output(model, X_test, y_test, sample_index):\n",
    "    sample = X_test[sample_index]\n",
    "    true_class = y_test[sample_index]\n",
    "    \n",
    "    input_value = list(map(Value, sample))\n",
    "    outputs = model(input_value)\n",
    "    predicted_class = np.argmax([p.data for p in outputs])\n",
    "\n",
    "    plt.imshow(sample.reshape(8, 8), cmap=plt.cm.gray_r)\n",
    "    plt.title(f\"True: {true_class}, Predicted: {predicted_class}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Show a random sample from the test set\n",
    "show_sample_with_output(neural_network, X_test, y_test, sample_index=random.randint(0, X_test.shape[0]-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd754bc",
   "metadata": {},
   "source": [
    "## Part 6: Putting It All Together with PyTorch\n",
    "\n",
    "You've done the hard work. You've built an entire automatic differentiation engine and neural network library from scratch. Now, let's see how modern deep learning frameworks like **PyTorch** allow us to do the same thing with much less code.\n",
    "\n",
    "This final task is to solve the same handwritten digit recognition problem, but this time using the tools provided by PyTorch. You will notice many parallels to the `Module`, `Layer`, and `Value` objects you created.\n",
    "\n",
    "First, you'll need to install PyTorch if you haven't already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d1175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/hodhod/miniconda3/lib/python3.13/site-packages (2.7.1+cu118)\n",
      "Requirement already satisfied: torchvision in /home/hodhod/miniconda3/lib/python3.13/site-packages (0.22.1+cu118)\n",
      "Requirement already satisfied: filelock in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: triton==3.3.1 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torch) (3.3.1)\n",
      "Requirement already satisfied: numpy in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torchvision) (2.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hodhod/miniconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae9194",
   "metadata": {},
   "source": [
    "### 6.1 Data Preparation with PyTorch\n",
    "\n",
    "PyTorch has its own data structures for handling data and creating batches efficiently. The first step is to convert our NumPy arrays into PyTorch `Tensors`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abe225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch shape: torch.Size([32, 64])\n",
      "Labels batch shape: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# TASK 6.1: Convert data to PyTorch Tensors and create DataLoaders.\n",
    "\n",
    "# --- Prepare the training data ---\n",
    "# 1. Convert training features and labels from NumPy arrays to PyTorch Tensors.\n",
    "#    Use torch.from_numpy(). Remember to convert them to the correct data type (.float() for features, .long() for labels).\n",
    "X_train_tensor = torch.from_numpy(X_train).float() # TODO\n",
    "y_train_tensor = torch.from_numpy(y_train).long() # TODO\n",
    "\n",
    "# 2. Create a TensorDataset to bundle features and labels together.\n",
    "train_dataset = TensorDataset(X_train_tensor , y_train_tensor)\n",
    "\n",
    "\n",
    "# 3. Create a DataLoader to handle batching and shuffling automatically.\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset,shuffle = True ,batch_size=batch_size) # TODO\n",
    "\n",
    "\n",
    "# --- Prepare the testing data ---\n",
    "# 4. Do the same for the test set.\n",
    "X_test_tensor = torch.from_numpy(X_test).float() # TODO\n",
    "y_test_tensor = torch.from_numpy(y_test).long() # TODO\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor) # TODO\n",
    "test_loader = DataLoader(test_dataset , batch_size = batch_size , shuffle = False) # TODO\n",
    "\n",
    "\n",
    "# Let's check a batch to see what the DataLoader gives us\n",
    "data_batch, labels_batch = next(iter(train_loader))\n",
    "print(f\"Data batch shape: {data_batch.shape}\")\n",
    "print(f\"Labels batch shape: {labels_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be86ddfa",
   "metadata": {},
   "source": [
    "### 6.2 Building the Model in PyTorch\n",
    "\n",
    "In PyTorch, models are created by creating a class that inherits from `nn.Module`. This is the direct equivalent of the `Module` class you built. Layers like `nn.Linear` are PyTorch's version of your `Layer` class, with weights and biases built-in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa946f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP_PyTorch(\n",
      "  (layer1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (layer2): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# TASK 6.2: Define the neural network architecture using nn.Module.\n",
    "\n",
    "class MLP_PyTorch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Define the layers of the network. Our network has:\n",
    "        # 1. A linear layer from 64 inputs to 32 outputs.\n",
    "        # 2. A ReLU activation function.\n",
    "        # 3. A final linear layer from 32 inputs to 10 outputs (for the 10 digits).\n",
    "        # You can try changing the architecture, but this is a good starting point.\n",
    "        # TODO: Define the layers as class attributes (e.g., self.layer1 = nn.Linear(...))\n",
    "        self.layer1 = nn.Linear(64,32)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(32,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This function defines the forward pass, just like your __call__ method.\n",
    "        # TODO: Pass the input 'x' through the layers you defined in __init__.\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = MLP_PyTorch()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff1a4f4",
   "metadata": {},
   "source": [
    "### 6.3 Defining the Loss and Optimizer\n",
    "\n",
    "PyTorch provides standard loss functions and optimizers. For this multi-class classification problem, `CrossEntropyLoss` is the best choice (it conveniently includes the Softmax layer for you). We'll use the Adam optimizer, which is a popular and effective choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7061e669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK 6.3: Instantiate the loss function and optimizer.\n",
    "\n",
    "# 1. The loss function for multi-class classification.\n",
    "criterion = nn.CrossEntropyLoss() # TODO: Use nn.CrossEntropyLoss()\n",
    "\n",
    "# 2. The optimizer that will update our model's parameters.\n",
    "#    It needs to know which parameters to optimize (model.parameters()).\n",
    "learning_rate = 0.0008\n",
    "optimizer = optim.Adam(model.parameters() ,lr=lr) # TODO: Use optim.AdamW() and pass it the model's parameters and the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b85cab",
   "metadata": {},
   "source": [
    "### 6.4 The Training Loop in PyTorch\n",
    "\n",
    "The training loop in PyTorch follows a standard, clean pattern. Compare this to the loop you wrote from scratch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaeb4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15, Loss: 1.3719\n",
      "Epoch 2/15, Loss: 0.7159\n",
      "Epoch 3/15, Loss: 0.5654\n",
      "Epoch 4/15, Loss: 0.4334\n",
      "Epoch 5/15, Loss: 0.4025\n",
      "Epoch 6/15, Loss: 0.3650\n",
      "Epoch 7/15, Loss: 0.2988\n",
      "Epoch 8/15, Loss: 0.3433\n",
      "Epoch 9/15, Loss: 0.2709\n",
      "Epoch 10/15, Loss: 0.3520\n",
      "Epoch 11/15, Loss: 0.2773\n",
      "Epoch 12/15, Loss: 0.3469\n",
      "Epoch 13/15, Loss: 0.3314\n",
      "Epoch 14/15, Loss: 0.3637\n",
      "Epoch 15/15, Loss: 0.5490\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# TASK 6.4: Complete the PyTorch training loop.\n",
    "\n",
    "n_epochs = 15\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # The standard 5-step PyTorch training process:\n",
    "\n",
    "        # 1. Zero the gradients from the previous iteration.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 2. Perform the forward pass to get model predictions.\n",
    "        outputs = model(X_batch)\n",
    "\n",
    "        # 3. Calculate the loss.\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        # 4. Perform the backward pass to compute gradients.\n",
    "        loss.backward()\n",
    "        \n",
    "        # 5. Update the model's weights.\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b51b1",
   "metadata": {},
   "source": [
    "### 6.5 Evaluating the PyTorch Model\n",
    "\n",
    "Finally, let's see how our new model performs on the test set. The evaluation loop is similar, but we tell PyTorch not to calculate gradients to save time and memory.\n",
    "\n",
    "*(This code is provided for you)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c196aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the network on the test images: 78.06 %\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Tell PyTorch not to track gradients during evaluation\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        # Get predictions\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # The class with the highest value is our prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'\\nAccuracy of the network on the test images: {accuracy:.2f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
